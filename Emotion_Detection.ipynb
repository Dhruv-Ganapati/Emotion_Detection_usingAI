{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhruv-Ganapati/Emotion_Detection_usingAI/blob/main/Emotion_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GlZT_pJjXzC"
      },
      "source": [
        "# Header: Emotion Detection:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwQ7-mhhjln-"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROzD4mqAXCvN",
        "outputId": "32087be1-42a1-4fe1-d128-34a5ba3def68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U81k4dEmkMsu",
        "outputId": "f3f31c0a-21ea-42bf-ce12-9f0926c8cd10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/AI-Udemy/Bonus Projects/Affectiva AI\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/AI-Udemy/Bonus Projects/Affectiva AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrpS-emfkrbM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "from IPython.display import display\n",
        "from tensorflow.keras import *\n",
        "from tensorflow.python.keras import *\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, optimizers\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import backend as K\n",
        "from keras import optimizers\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWhP88ktABLe"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57NNJgtVCD2l"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiYh8aodCHoI"
      },
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vc0nKubuCOc4"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2czgmK8wCWDn"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQtsb4WqCcqO"
      },
      "outputs": [],
      "source": [
        "df.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypZDwgDkCwBl"
      },
      "source": [
        "Here, 30 columns are Floating type data set and Remaining 1 data set is image(Object type).\n",
        "\n",
        "Images contains values of it's color.\n",
        "* 0 : represent full balck object\n",
        "* 255: represent white image object\n",
        "* Between 0-255: represent Gray-scale image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF15o3aZCh8B"
      },
      "outputs": [],
      "source": [
        "df['Image']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTtP6l4kDFnA"
      },
      "outputs": [],
      "source": [
        "df['Image'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itZhrNXWDu2Y"
      },
      "outputs": [],
      "source": [
        "df['Image'].size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSUYyG4oD0lV"
      },
      "outputs": [],
      "source": [
        "# Since values for the image are given as space separated string, separate the values using ' ' as separator.\n",
        "# Then convert this into numpy array using np.fromstring and convert the obtained 1D array into 2D array of shape (96, 96)\n",
        "df['Image'] = df['Image'].apply(lambda x: np.fromstring(x, dtype = int, sep=' ').reshape(96,96))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ofd29bwEkw3"
      },
      "outputs": [],
      "source": [
        "df['Image'][0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOJ1kytlYlZi"
      },
      "source": [
        "MINI CHALLENGE #1:\n",
        "- Obtain the average, minimum and maximum values for 'right_eye_center_x'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih6xRQJAYLbr"
      },
      "outputs": [],
      "source": [
        "df['right_eye_center_x'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W90NwiZ5Yqba"
      },
      "source": [
        "### =========================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gza7WSsBYuxi"
      },
      "source": [
        "# PERFORM IMAGE VISUALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aIixCOVYfJo"
      },
      "outputs": [],
      "source": [
        "# Plot a random image from the dataset along with facial keypoints.\n",
        "# Image data is obtained from df['Image'] and plotted using plt.imshow\n",
        "# 15 x and y coordinates for the corresponding image\n",
        "# since x-coordinates are in even columns like 0,2,4,.. and y-coordinates are in odd columns like 1,3,5,..\n",
        "# we access their value using .loc command, which get the values for coordinates of the image based on the column it is refering to.\n",
        "\n",
        "np.random.seed(78)    # For same image, seed helps to provide same  random value everytime\n",
        "i = np.random.randint(1, len(df))\n",
        "plt.imshow(df['Image'][i], cmap='gray')\n",
        "for j in range(1,31,2):\n",
        "  plt.plot(df.loc[i][j-1], df.loc[i][j], 'rx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNTxz6VSdvNV"
      },
      "source": [
        "* <!-- i =[]\n",
        "for j in range(1, 31, 2):\n",
        "  i.append(j)\n",
        "  print(i) -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qM-QVa0qZUdb"
      },
      "outputs": [],
      "source": [
        "#View more images in grid format\n",
        "fig = plt.figure(figsize=(20,20),dpi=200)\n",
        "\n",
        "i=[]\n",
        "# for m in range(16):\n",
        "#   i.append(np.random.randint(m,len(df)))    # Generate random images and stores on I[] list\n",
        "\n",
        "# Generate 16 unique random numbers\n",
        "while len(i) < 16:\n",
        "    random_number = np.random.randint(0, len(df))  # Generate a random number\n",
        "    if random_number not in i:   # Check if the number is unique\n",
        "        i.append(random_number)  # Append unique random number\n",
        "print(\"I: \", i)\n",
        "\n",
        "for a in range(16):\n",
        "  ax = fig.add_subplot(4,4,a+1)           # provide 16 empty image boxes\n",
        "  a = i[a]                        # Putting random images loc on loop\n",
        "  image = plt.imshow(df[\"Image\"][a], cmap='gray')   # FIll all the related images on the boxes\n",
        "  for j in range(1, 31, 2):\n",
        "    plt.plot(df.loc[a][j-1], df.loc[a][j], 'rx')     # Fill the red 'x' facial point.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38kuoIT61P08"
      },
      "source": [
        "MINI CHALLENGE #2:\n",
        "- Perform a sanity check on the data by randomly visualizing 64 new images along with their cooresponding key points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-NQAfZinDlF"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "fig = plt.figure(figsize=(20,20))\n",
        "\n",
        "x = []\n",
        "while len(x) < 64:\n",
        "  random_number = np.random.randint(0, len(df))\n",
        "  if random_number not in x:\n",
        "    x.append(random_number)\n",
        "\n",
        "print(x)    # Generated 64 unique random number from data frame\n",
        "\n",
        "for a in range(64):\n",
        "  ax = fig.add_subplot(8,8, a+1)        # Here, 8,8 is 8 rows * 8 columns\n",
        "  a = x[a]\n",
        "  image = plt.imshow(df['Image'][a], cmap='gray')\n",
        "  for j in range(1,31,2):\n",
        "    plt.plot(df.loc[a][j-1], df.loc[a][j], 'rx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4w88LR8AoN4"
      },
      "source": [
        "# PERFORM IMAGE AUGMENTATION\n",
        "Image augmentation is a technique commonly used in deep learning for computer vision tasks, such as image classification, object detection, and segmentation. It involves applying various transformations to the original images in the training dataset to create new training examples.\n",
        "Some common image augmentation techniques include:\n",
        "\n",
        "* Rotation: Rotating the image by a certain angle.\n",
        "* Flip: Flipping the image horizontally or vertically.\n",
        "* Zoom: Zooming in or out of the image.\n",
        "* Translation: Shifting the image horizontally or vertically.\n",
        "* Brightness adjustment: Changing the brightness level of the image.\n",
        "* Contrast adjustment: Adjusting the contrast of the image.\n",
        "* Noise injection: Adding random noise to the image.\n",
        "* Crop and resize: Cropping a portion of the image and resizing it to the original size.\n",
        "* Shear: Applying shearing transformations to the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52kq5Qzs2W6o"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "copy_df = copy.copy(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIxXbldlin7a"
      },
      "source": [
        "Generalization of model is very important than memorization. One can't memorise all the thing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44crTEvLiZFK"
      },
      "outputs": [],
      "source": [
        "# showing all the key facial point of the dataFrame\n",
        "columns = copy_df.columns[:-1]\n",
        "columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7HQDOWKyku3"
      },
      "source": [
        "###**Horizontal Flip**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DMlKrmLjz7Z"
      },
      "outputs": [],
      "source": [
        "# Fliping image Horizontally\n",
        "copy_df['Image'] = copy_df['Image'].apply(lambda x:np.flip(x, axis=1))\n",
        "\n",
        "##since we are fliping Horizonatlly, y co-ordinates values would be the same.\n",
        "# Only x-coordinates values would change, all we have to do is to substract our initial x-coordinates values form width of the image(96*96)\n",
        "for i in range(len(columns)):\n",
        "  if i%2 == 0:          # only even means: x-coordinates\n",
        "    copy_df[columns[i]] = copy_df[columns[i]].apply(lambda x: 96.0 -float(x))      # 96 pixels -( x-cordinates)value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxfMSUlWxoSz"
      },
      "outputs": [],
      "source": [
        "df['left_eye_center_x']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YfCCI8kxq2B"
      },
      "outputs": [],
      "source": [
        "# Here value = 96 - df(value)\n",
        "copy_df['left_eye_center_x']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iZANt_Qv10X"
      },
      "outputs": [],
      "source": [
        "# Showing the original images as:\n",
        "plt.imshow(df['Image'][0], cmap='gray')\n",
        "for j in range(1, 31, 2):\n",
        "  plt.plot(df.loc[0][j-1], df.loc[0][j], 'rx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PqSgr-CwHI6"
      },
      "outputs": [],
      "source": [
        "# Showing the Flipped image = Horizontal flipped image\n",
        "plt.imshow(copy_df['Image'][0], cmap='gray')\n",
        "for j in range(1, 31, 2):\n",
        "  plt.plot(copy_df.loc[0][j-1], copy_df.loc[0][j], 'rx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntiFq9Qz3c5P"
      },
      "source": [
        "**Augmented DataFrame** -concate(original + Horizontally flipped data)\n",
        "* Note: The more we do augmentation, the data become more complicated.\n",
        "\n",
        "\n",
        "Here we increase the brightness fo this image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NedYYpZYwfge"
      },
      "outputs": [],
      "source": [
        "  augmented_df = np.concatenate((df, copy_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNfuEvl03pBP"
      },
      "outputs": [],
      "source": [
        "augmented_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyd9Q7k2Ai2F"
      },
      "source": [
        "**Brightness: Increase**\n",
        "* np.clip: \"Here CLIP helps to bring pixels values to the range between 0.0 to 255.0, if the pixel values goes out of range.\n",
        "      * For eg: suppose x=200 pix\n",
        "        then 200 * 2 = 400 pix which is out of range(0-255). Then np.CLIP bring it between range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVV3VuBe3pDu"
      },
      "outputs": [],
      "source": [
        "# Randomly increasing the brightness of the image\n",
        "# We multiply pixel values by random values range(1.5 -2.0) to incease the brightness\n",
        "# We can clip the value between 0-255\n",
        "import random\n",
        "\n",
        "copy_df = copy.copy(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSwdcrh03pF4"
      },
      "outputs": [],
      "source": [
        "copy_df['Image'] = copy_df['Image'].apply(lambda x:np.clip(random.uniform(1.5,2)*x, 0.0, 255.0))\n",
        "augmented_df = np.concatenate((augmented_df, copy_df))\n",
        "augmented_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjEGc_Wi3pIP"
      },
      "outputs": [],
      "source": [
        "# show the image\n",
        "plt.imshow(copy_df['Image'][0], cmap='gray')\n",
        "for j in range(1,31,2):\n",
        "  plt.plot(copy_df.loc[0][j-1], copy_df.loc[0][j], 'rx')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25_gVKa83pNc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZyPfKI1yVld"
      },
      "source": [
        "###**Vertical Flip**\n",
        "Challenge MINI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yl0s0nx8yYYl"
      },
      "outputs": [],
      "source": [
        "ver_df = copy.copy(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0L66M3U0fpQ"
      },
      "outputs": [],
      "source": [
        "cloumns = ver_df.columns[:-1]\n",
        "columns.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ptkz8dJ5zaAu"
      },
      "outputs": [],
      "source": [
        "#Vertical Flip [axis=0] - flip the image along x-axis\n",
        "ver_df['Image'] = ver_df['Image'].apply(lambda x: np.flip(x, axis=0))\n",
        "\n",
        "# Since we are flipping vertically, x-cordinates values would be same\n",
        "# Only y-cordinates values would changes, all we to do is to substract our initial y-cordinates values from height:96\n",
        "for i in range(len(columns)):\n",
        "  if i % 2 == 1:\n",
        "    ver_df[columns[i]] = ver_df[columns[i]].apply(lambda x: 96.0 -float(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1O4h-Q9z0a1"
      },
      "outputs": [],
      "source": [
        "plt.imshow(ver_df['Image'][0], cmap='gray')\n",
        "#show 'x' red points\n",
        "for i in range(1, 31, 2):\n",
        "  plt.plot(ver_df.loc[0][i-1], ver_df.loc[0][i], 'rx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjYtoRrZ1gbw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7tIR6xLsm4n"
      },
      "source": [
        "## PERFORMING DATA NORMALIZATION & taining data\n",
        "* AUGMENTED Data Frame is in the form of NUMPY Array. So we can't extact images column by\n",
        "* augmented_df['Image']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yVczWEZs15a"
      },
      "outputs": [],
      "source": [
        "# Obtain the value of images which is present in the 31st columns (Since Index start from 0, we refer to 31st column)\n",
        "img = augmented_df[:, 30]\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHrDd35pycId"
      },
      "source": [
        "**Image Normalization:**\n",
        "Image normalization is a common preprocessing step in computer vision tasks for several reasons:\n",
        "\n",
        "1. **Better Model Convergence**: Normalizing pixel values to a fixed range, such as [0, 1] or [-1, 1], can help improve the convergence of optimization algorithms during training. This is because normalizing the input data reduces the scale of input features, which can lead to more stable training dynamics and faster convergence.\n",
        "\n",
        "2. **Reduced Sensitivity to Scale**: Neural networks can be sensitive to the scale of input features. Normalizing pixel values to a fixed range helps mitigate this sensitivity, making the model more robust to changes in the scale of input data.\n",
        "\n",
        "3. **Improved Generalization**: Normalizing images can help the model generalize better to unseen data by reducing the impact of variations in pixel intensity across different images. This can lead to better performance on validation and test sets.\n",
        "\n",
        "4. **Avoiding Saturation**: Neural networks often use activation functions like sigmoid or tanh, which saturate at extreme values. By normalizing pixel values to a fixed range, you can avoid saturation of these activation functions and ensure that gradients flow effectively during training.\n",
        "\n",
        "5. **Compatibility with Pretrained Models**: Many pretrained models and architectures, especially those trained on large-scale datasets like ImageNet, expect input images to be normalized in a specific way. By normalizing your input images to match the expectations of these models, you can effectively use transfer learning and leverage pretrained weights for your own tasks.\n",
        "\n",
        "Overall, image normalization is a simple yet effective preprocessing technique that can improve the stability, convergence, and generalization of neural network models in computer vision tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnNoIHSptFNX"
      },
      "outputs": [],
      "source": [
        "# Normalize the images\n",
        "img = img / 255       # dividing by highest pixel\n",
        "\n",
        "# create an empty array of shape (x, 96, 96, 1) to feed the model\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPzxAMs7tSqL"
      },
      "outputs": [],
      "source": [
        "# Create an empty array of shape(x, 96, 96, 1) to feed the model\n",
        "X = np.empty((len(img), 96, 96, 1))\n",
        "\n",
        "# Iterate through the img list and add image values to the empty aray after expanding it's dimension from (96, 96) to (96, 96, 1)\n",
        "for i in range(len(img)):\n",
        "  X[i, ] = np.expand_dims(img[i], axis=2)\n",
        "\n",
        "# Convert the images to float32 to ensure division results in float values\n",
        "X = np.array(X).astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ngwt-0n9tWFb"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtAxb5RC6OuI"
      },
      "outputs": [],
      "source": [
        "# Obtain the values of x and y co-ordinates which are used as target\n",
        "y = augmented_df[:, :30]      # excluding 31st column\n",
        "y = np.asarray(y).astype(np.float32)\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALuWFeU863fB"
      },
      "outputs": [],
      "source": [
        "#X[0][0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBA60b6g7LrC"
      },
      "source": [
        "**TRAIN & TEST: SPLIT DATA**\n",
        "* X_test and X_train: INput for testing and training data\n",
        "* y_test and y_train: Output fo testing and taining data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVa4MOHP66P7"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cAKe8pw7nZq"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bfs7-u2g7p3D"
      },
      "outputs": [],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpcQ9hDw7seb"
      },
      "outputs": [],
      "source": [
        "y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnEbgm9M7vw7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fOWZ7SOAEr3"
      },
      "source": [
        "### BUILDING DEEP RESIDUAL NEURAL NETWORK KEY FACIAL POINTS DETECTION MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy5CbYQSAFkz"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1q0If_tsIyz2GfOb9EsRH_8_ue-1QnTlu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMOoyrd3_WHw"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1GDhehqRRtnTA3-i02cYcalbPA27ej7Ar)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "torP9JFY_WLN"
      },
      "source": [
        "**Creating RES-BLOCK**\n",
        "* 1. CONVOLUTION BLOCK\n",
        "* 2. IDENTITY BLOCK\n",
        "* 3. IDENTITY BLOCK\n",
        "\n",
        "Let's first create CONVOLUTION BLOCK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXcEJHkP_WZ2"
      },
      "outputs": [],
      "source": [
        "def res_block(X, filter, stage):\n",
        "\n",
        "  ##CONVOLUTION BLOCK\n",
        "  X_copy = X\n",
        "  f1, f2, f3 = filter\n",
        "\n",
        "  #main path ---------------------------------------PART 1\n",
        "  # 1. Conv2D\n",
        "  # padding valid : default padding\n",
        "  X= Conv2D(f1, kernel_size=(1,1), strides=(1,1), name='res_'+str(stage)+'_conv_a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "\n",
        "  #2. MaxPool2D\n",
        "  X = MaxPool2D(pool_size=(2,2))(X)\n",
        "\n",
        "  #3. BatchNorm, Relu\n",
        "  X = BatchNormalization(axis=3, name=\"bn_\"+str(stage)+\"_conv_a\")(X)\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  #4. Conv2D - kernel(3*3)\n",
        "  # Padding same : In this type of padding, we only append zero to the left of the array and to the top of the 2D input matrix.\n",
        "  X = Conv2D(f2, kernel_size=(3,3), strides=(1,1), padding='same', name='res_'+str(stage)+'_conv_b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "\n",
        "  #5. BatchNorm, Relu\n",
        "  X = BatchNormalization(axis=3, name='bn_'+str(stage)+'_conv_b')(X)\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  #6. Conv2D\n",
        "  X = Conv2D(f3, kernel_size=(1,1), strides=(1,1), name='res_'+str(stage)+'_conv_c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "  X = BatchNormalization(axis=3, name='bn_'+str(stage)+'_conv_c')(X)\n",
        "\n",
        "\n",
        "\n",
        "  #short path ---------------------------------------PART 2\n",
        "  #1. Conv2D\n",
        "  X_copy = Conv2D(f3, (1,1), (1,1), name='res_'+str(stage)+'_conv_copy', kernel_initializer=glorot_uniform(seed=0))(X_copy)\n",
        "\n",
        "  #2. MaxPool2D\n",
        "  X_copy = MaxPool2D(pool_size=(2,2))(X_copy)\n",
        "\n",
        "  #3. BatchNorm\n",
        "  X_copy = BatchNormalization(axis=3, name='bn_'+str(stage)+'_conv_copy')(X_copy)\n",
        "\n",
        "  # ADDING BOTH PATH\n",
        "  X = Add()([X, X_copy])\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "\n",
        "\n",
        "  ##IDENTITY BLOCK 1\n",
        "  X_copy = X\n",
        "\n",
        "  #main path\n",
        "  #1. Conv2D\n",
        "  X = Conv2D(f1, kernel_size=(1,1), strides=(1,1), name='res_'+str(stage)+'_identity_1_a', kernel_initializer=glorot_uniform(seed =0))(X)\n",
        "\n",
        "  #2. BatchNorm, relu\n",
        "  X = BatchNormalization(axis=3, name='bn_'+str(stage)+'_identity_1_a')(X)\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  #3. Conv2D - kernel(3*3)\n",
        "  X = Conv2D(f2, kernel_size=(3,3), strides=(1,1), padding='same', name='res_'+str(stage)+'_identity_1_b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "\n",
        "  #4. BatchNorm, relu\n",
        "  X = BatchNormalization(axis=3, name='bn_'+str(stage)+'_identity_1_b')(X)\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  #5. Conv2D\n",
        "  X = Conv2D(f3, (1,1), (1,1), name='res_'+str(stage)+'_identity_1_c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "\n",
        "  #6. BatchNorm\n",
        "  X = BatchNormalization(axis=3, name='bn_'+str(stage)+'_identity_1_c')(X)\n",
        "\n",
        "  # Add\n",
        "  X = Add()([X, X_copy])\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "\n",
        "  ##IDENTITY BLOCK 2\n",
        "  X_copy = X\n",
        "\n",
        "  #main path\n",
        "  #1. Conv2D\n",
        "  X = Conv2D(f1, kernel_size=(1,1), strides=(1,1), name='res_'+str(stage)+'_identity_2_a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "  #2. BatchNorm, relu\n",
        "  X = BatchNormalization(axis=3, name='bn_'+str(stage)+'_identity_2_a')(X)\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  #3. Conv2D -kernel(3*3)\n",
        "  X = Conv2D(f2, (3,3), (1,1), padding='same', name='res_'+str(stage)+'_identity_2_b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "  #4. BatchNOrm, relu\n",
        "  X = BatchNormalization(axis=3, name='bn_'+str(stage)+'_identity_2_b')(X)\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  #5. Conv2D\n",
        "  X = Conv2D(f3, (1,1), (1,1), name='res_'+str(stage)+'_identity_2_c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "  #6. BatchNorm\n",
        "  X = BatchNormalization(axis = 3, name='bn_'+str(stage)+'_identity_2_c')(X)\n",
        "\n",
        "  # Add\n",
        "  X = Add()([X, X_copy])\n",
        "  A = Activation('relu')(X)\n",
        "\n",
        "  return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbd4rFi3fN_U"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1G_gvfoKJyRGpq_oNejLwDj7ZxIZvCGk6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-8aHpI0f1MW"
      },
      "source": [
        "### Creating FINAL MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mePJshX9_Wc4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, ZeroPadding2D, Conv2D, BatchNormalization, Activation, MaxPooling2D, AveragePooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "input_shape = (96,96,1)\n",
        "\n",
        "#1. Input Tensor shape\n",
        "X_input = Input(input_shape)\n",
        "\n",
        "#2. ZeroPadding\n",
        "X = ZeroPadding2D((3,3))(X_input)\n",
        "\n",
        "#3. Conv2D, BatchNOrm, MaxPool2D\n",
        "X = Conv2D(64, kernel_size=(7,7), strides=(2,2), name='conv1', kernel_initializer=glorot_uniform(seed = 0))(X)\n",
        "X = BatchNormalization(axis=3, name= 'bn_conv1')(X)\n",
        "X = Activation('relu')(X)\n",
        "X = MaxPooling2D((3,3), strides=(2,2))(X)\n",
        "\n",
        "#4. RES_BLOCK\n",
        "X = res_block(X, filter=[64, 64, 256], stage=2)\n",
        "X = res_block(X, filter= [128, 128, 512], stage=3)\n",
        "\n",
        "#5. AveragePooling2D\n",
        "X = AveragePooling2D((2,2), name='Average_Pooling')(X)\n",
        "\n",
        "#6. Final Layer(Flatten(), [Dense Layer-Relu, Dropout]*2, Dense Layer-Relu)\n",
        "X =Flatten()(X)\n",
        "X = Dense(4096, activation = 'relu')(X)\n",
        "X = Dropout(0.2)(X)\n",
        "\n",
        "X = Dense(2048, activation='relu')(X)\n",
        "X = Dropout(0.1)(X)\n",
        "\n",
        "X = Dense(30, activation = 'relu')(X)\n",
        "\n",
        "\n",
        "\n",
        "model_1_facialKeyPoints = Model(inputs = X_input, outputs=X)\n",
        "model_1_facialKeyPoints.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVS8HLEr_Wfb"
      },
      "outputs": [],
      "source": [
        "model_1_facialKeyPoints.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtaKk49N_Wk4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOX2eR3R8YPH"
      },
      "source": [
        "### COMPLIE & TRAIN KEY FACIAL POINTS DETECTION deep learning model\n",
        "\n",
        "* adam = tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    weight_decay=None,\n",
        "    clipnorm=None,\n",
        "    clipvalue=None,\n",
        "    global_clipnorm=None,\n",
        "    use_ema=False,\n",
        "    ema_momentum=0.99,\n",
        "    name='adam'\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpl59thy8R_A"
      },
      "outputs": [],
      "source": [
        "adam = tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.0001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    weight_decay=None,\n",
        "    clipnorm=None,\n",
        "    clipvalue=None,\n",
        "    global_clipnorm=None,\n",
        "    use_ema=False,\n",
        "    ema_momentum=0.99,\n",
        "    name='adam'\n",
        "  )\n",
        "model_1_facialKeyPoints.compile(loss=\"mean_squared_error\", optimizer=adam, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Re96ZWyRg8A"
      },
      "outputs": [],
      "source": [
        "# using early stopping to exit training if validation loss is not decreasing even after certain epochs (patience)\n",
        "earlystopping = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 20)\n",
        "\n",
        "# save the best model with least validation loss\n",
        "checkpointer = ModelCheckpoint(filepath='FacialKeyPoints_weight.h5', mode = 'min', verbose=1, save_best_only=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAB5kisZODkK"
      },
      "outputs": [],
      "source": [
        "history1 = model_1_facialKeyPoints.fit(X_train, y_train, batch_size = 32, epochs = 100, validation_split = 0.01, callbacks=[checkpointer, earlystopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGydGkInYRha"
      },
      "outputs": [],
      "source": [
        "# save the model architecture to json file for future use\n",
        "\n",
        "model_json = model_1_facialKeyPoints.to_json()\n",
        "with open(\"FacialKeyPoints-model.json\", \"w\") as json_file:\n",
        "  json_file.write(model_json)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeEFvJBttw4B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3O3XZYO_D9E"
      },
      "source": [
        "## ASSESS TRANIED KEY FACIAL POINTS DETECTION MODEL PERFORMANCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkSrQ72a8g4X"
      },
      "outputs": [],
      "source": [
        "from keras.models import model_from_json\n",
        "\n",
        "# Load the model architecture from the JSON file\n",
        "with open(\"FacialKeyPoints-model.json\", \"r\") as json_file:\n",
        "    loaded_model_json = json_file.read()\n",
        "\n",
        "# Reconstruct the model from the loaded architecture (Loaded_model = model_1_facialKeyPoint ho)\n",
        "model_1_facialKeyPoints = model_from_json(loaded_model_json)\n",
        "\n",
        "# Load the trained weights\n",
        "model_1_facialKeyPoints.load_weights(\"FacialKeyPoints_weight.h5\")\n",
        "\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "# Compile the loaded model (if needed)\n",
        "model_1_facialKeyPoints.compile(optimizer=adam, loss='mse', metrics=['accuracy'])\n",
        "\n",
        "# Use the loaded model for inference or further training\n",
        "# For example:\n",
        "# predictions = model_1_facialKeyPoint.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9hmBBsKpP6_"
      },
      "outputs": [],
      "source": [
        "result = model_1_facialKeyPoints.evaluate(X_test, y_test)\n",
        "print(\"Accuracy: {}\".format(result[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghiOdxE7NAoV"
      },
      "source": [
        "So, Here we got the accuracy of 74.143% on unseen testing data which is pretty good.\n",
        "# Grid Search CV\n",
        "### How to increase accuracy rate:\n",
        "code :\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "- Define the model\n",
        "- Replace 'model' with your model architecture\n",
        "model = ...\n",
        "\n",
        "- Define the parameter grid\n",
        "param_grid = {'lr': [0.001, 0.01, 0.1]}  # Learning rate values to try\n",
        "\n",
        "- Define the optimizer\n",
        "optimizer = Adam()\n",
        "\n",
        "- Compile the model\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "- Define GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
        "\n",
        "- Train the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "- Get the best parameters\n",
        "best_lr = grid_search.best_params_['lr']\n",
        "\n",
        "- Use the best learning rate to train the model\n",
        "optimizer = Adam(learning_rate=best_lr)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_split=0.01)\n",
        "\n",
        "- Evaluate on test data\n",
        "result = model.evaluate(X_test, y_test)\n",
        "print(\"Accuracy: {}\".format(result[1]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAaXre28JzeK"
      },
      "outputs": [],
      "source": [
        "history1.history.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guVPgnAQJ-0V"
      },
      "outputs": [],
      "source": [
        "np.max(history1.history['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZAnjfBELRfv"
      },
      "outputs": [],
      "source": [
        "# Create subplots with 1 rows and 2 column\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "axes[0].plot(history1.history['accuracy'])\n",
        "axes[0].plot(history1.history['val_accuracy'])\n",
        "axes[0].set_title('Model accuracy')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].legend(['Train_Acc', 'Validation_Acc'], loc='upper left')\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Subplot for loss\n",
        "axes[1].plot(history1.history['loss'])\n",
        "axes[1].plot(history1.history['val_loss'])\n",
        "axes[1].set_title('Model loss')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].legend(['Train_loss', 'Validation_loss'], loc='upper left')\n",
        "axes[1].grid(True, color='gray')\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7NtdwKTL8mn"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorflow scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fofouzKQ33qm"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "# # Define a function to create the Keras model\n",
        "# def create_model(lr=0.01):\n",
        "#     model = Sequential([\n",
        "#         Dense(64, activation='relu', input_shape=(input_shape,)),  # Replace input_shape with your input shape\n",
        "#         Dense(32, activation='relu'),\n",
        "#         Dense(num_classes, activation='softmax')  # Replace num_classes with the number of output classes\n",
        "#     ])\n",
        "#     optimizer = Adam(learning_rate=lr)\n",
        "#     model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#     return model\n",
        "\n",
        "# # Define the parameter grid\n",
        "# param_grid = {'lr': [0.001, 0.01, 0.1]}  # Learning rate values to try\n",
        "\n",
        "# # Create the Keras model\n",
        "# keras_model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# # Define GridSearchCV\n",
        "# grid_search = GridSearchCV(estimator=keras_model, param_grid=param_grid, cv=3)\n",
        "\n",
        "# # Train the model\n",
        "# grid_search.fit(X_train, y_train)\n",
        "\n",
        "# # Get the best parameters\n",
        "# best_lr = grid_search.best_params_['lr']\n",
        "\n",
        "# # Use the best learning rate to train the model\n",
        "# best_model = create_model(lr=best_lr)\n",
        "# history = best_model.fit(X_train, y_train, epochs=4, batch_size=64, validation_split=0.01)\n",
        "\n",
        "# # Evaluate on test data\n",
        "# result = best_model.evaluate(X_test, y_test)\n",
        "# print(\"Accuracy: {}\".format(result[1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzyBAVcHbSdK"
      },
      "outputs": [],
      "source": [
        "#!pip install --upgrade --force-reinstall keras\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSEXSJzCcjX0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vz-ToU16vWJi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUb5KhUlvT7h"
      },
      "source": [
        "**=================================================PART -2=======================================================**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRRvOAXHvtdr"
      },
      "source": [
        "### **FACIAL EXPRESSION(EMOTIOIN) DETECTION**\n",
        "\n",
        "- DATA importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_IsGB3-vfkd"
      },
      "outputs": [],
      "source": [
        "facial_df = pd.read_csv('icml_face_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2rxRjz9xKMg"
      },
      "outputs": [],
      "source": [
        "facial_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISZVd4jZyPrT"
      },
      "outputs": [],
      "source": [
        "# @title null value\n",
        "facial_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSjzDlJZyPxq"
      },
      "outputs": [],
      "source": [
        "facial_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1DcSJyLyQDE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wv7DB3MVxUgO"
      },
      "outputs": [],
      "source": [
        "# @title emotion\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "facial_df['emotion'].plot(kind='hist', bins=20, title='emotion')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QA_r6HZyzlP"
      },
      "source": [
        "**String -> array**\n",
        "Everything all right. Let's convert string pixel into array format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bussnjNxqTo"
      },
      "outputs": [],
      "source": [
        "facial_df[' pixels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xh4T46Ew0Ar1"
      },
      "outputs": [],
      "source": [
        "facial_df['emotion'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGQD5SAn0J-P"
      },
      "outputs": [],
      "source": [
        "# function to convert pixel values in string format to array format\n",
        "def string2array(x):\n",
        "  return np.array(x.split(' ')).reshape(48,48,1).astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eb4eII0L5tzY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7HoZ9510_-5"
      },
      "outputs": [],
      "source": [
        "# function to resize images from (48, 48) to (96, 96)\n",
        "def resize(x):\n",
        "  img = x.reshape(48, 48)\n",
        "  return cv2.resize(img, dsize=(96,96), interpolation=cv2.INTER_LINEAR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCcqzaUd1pz9"
      },
      "outputs": [],
      "source": [
        "facial_df[' pixels'] = facial_df[' pixels'].apply(lambda x: string2array(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vfTc5wq1-a8"
      },
      "outputs": [],
      "source": [
        "facial_df[' pixels'] = facial_df[' pixels'].apply(lambda x: resize(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTzH2k5C2MP0"
      },
      "outputs": [],
      "source": [
        "facial_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD8UeHKh2_Sx"
      },
      "outputs": [],
      "source": [
        "facial_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SL1omAKP5Kaf"
      },
      "outputs": [],
      "source": [
        "# @title rename column\n",
        "facial_df = facial_df.rename(columns={' pixels': 'pixels'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffol8mIz5vJZ"
      },
      "outputs": [],
      "source": [
        "facial_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Orf0On-1cMNz"
      },
      "source": [
        "**Visualize the any image in the dataframe**\n",
        "\n",
        "The image will be low pixels because we have converted it into (96,96) from low pixels value (48,48)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnF9uvvLcg7V"
      },
      "outputs": [],
      "source": [
        "plt.imshow(facial_df['pixels'].iloc[-1], cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpOcscCldQPE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kzBKCq3eeDG"
      },
      "source": [
        "### **VISUALIZE IMAGES AND PLOT LABELS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJBmDUzZej3P"
      },
      "outputs": [],
      "source": [
        "label_to_text = {0:'anger', 1:'disgust', 2:'sad', 3:'happiness', 4:'surprise'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unUtUxITe02O"
      },
      "outputs": [],
      "source": [
        "emotions = [0, 1, 2, 3, 4]\n",
        "\n",
        "for i in emotions:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GZRtNkGDHoV"
      },
      "outputs": [],
      "source": [
        "# @title Index Y-Axis\n",
        "facial_df.emotion.value_counts().index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4a6AYJqE1Xq"
      },
      "outputs": [],
      "source": [
        "# @title Data X-Axis\n",
        "facial_df['emotion'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hq5DsLxHFDjO"
      },
      "outputs": [],
      "source": [
        "# @title BarPlot\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.barplot(y= facial_df.emotion.value_counts(),x=facial_df.emotion.unique(), color='red')\n",
        "plt.title(\"LABELLING DATA\")\n",
        "plt.xlabel(label_to_text)\n",
        "plt.ylabel(\"No.of Samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqHPi-NzHsjx"
      },
      "source": [
        "**Here, the sample data for *Label:1:Disgust* is very Low. The model will not be benefitted from this. Less data means low reading and less efficient model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "padwD6Vs2Px1"
      },
      "source": [
        "## **PERFORM IMAGE AUGMENTATION & DATA  PREPERATIONt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNWHaIJ-FtLZ"
      },
      "outputs": [],
      "source": [
        "# split the dataframe\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "X = facial_df['pixels']\n",
        "y = facial_df['emotion']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ae1Ovn9V271g"
      },
      "outputs": [],
      "source": [
        "y = to_categorical(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDU5pxNy3CF4"
      },
      "outputs": [],
      "source": [
        "y[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XedQD7D3Dkv"
      },
      "outputs": [],
      "source": [
        "X[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6kyeczN3WK8"
      },
      "outputs": [],
      "source": [
        "# Stack : each element of X is an array of the same shape, and it stacks them along the new axis to create a new array.The resulting array X would have one additional dimension compared to the original arrays in X.\n",
        "X = np.stack(X, axis=0)\n",
        "X = X.reshape(24568, 96, 96, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UvkzxV15B-z"
      },
      "outputs": [],
      "source": [
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COmsTK_s5ep9"
      },
      "source": [
        "(24568, 96, 96, 1): This show, there are 24k images having 96 * 96 pixels with only 1 color channel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAubumxs4h75"
      },
      "outputs": [],
      "source": [
        "# Train, Test and Validation dataSet\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True) # training : 80%, test : 20%\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, shuffle=True)   # validation : 50% of test, test : 50% of test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUxEAfkl6nze"
      },
      "source": [
        "**Shuffle:**shuffle determines whether the data should be shuffled before splitting it into training and testing sets.\n",
        "* In most cases, setting shuffle to True is recommended, as it helps in producing more robust and generalizable models by preventing any biases introduced by the original order of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfzB7irL5BKB"
      },
      "outputs": [],
      "source": [
        "print(X_val.shape, y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESksKEIy7mqq"
      },
      "outputs": [],
      "source": [
        "print(X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gypO9CkN72pY"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TG89G5k77M5"
      },
      "outputs": [],
      "source": [
        "(24568 - 24568 * 0.8) / 2 # validation & test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asNy9NDP8Zwm"
      },
      "outputs": [],
      "source": [
        "# @title image-processing : Image Normalization ranges(0-1)\n",
        "X_train = X_train/255\n",
        "X_val = X_val / 255\n",
        "X_test = X_test / 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmv29BcgBBiK"
      },
      "source": [
        "**CODE:**\n",
        "import numpy as np\n",
        "import imgaug.augmenters as iaa\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "* Load an image\n",
        "image = np.array(Image.open(\"example_image.jpg\"))\n",
        "\n",
        "* Define the cutout augmentation\n",
        "augmenter = iaa.Cutout(nb_iterations=(1, 5), size=(0.1, 0.2), squared=False)\n",
        "\n",
        "* Apply augmentation to the image\n",
        "augmented_image = augmenter(image=image)\n",
        "\n",
        "* Plot the original and augmented images\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "axes[0].imshow(image)\n",
        "axes[0].set_title(\"Original Image\")\n",
        "axes[0].axis(\"off\")\n",
        "axes[1].imshow(augmented_image)\n",
        "axes[1].set_title(\"Augmented Image with Cutout\")\n",
        "axes[1].axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jl5B9LNR8zJD"
      },
      "outputs": [],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHYkfrGSBXVg"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range =15,\n",
        "    width_shift_range = 0.1,\n",
        "    height_shift_range = 0.1,\n",
        "    shear_range = 0.1,\n",
        "    zoom_range = 0.1,\n",
        "    horizontal_flip = True,\n",
        "    fill_mode = 'nearest'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDZkH582RoKa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qd9W2aYDP9HD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPlc3uYzRSwx"
      },
      "source": [
        "## **BUILD AND TRAIN DEEP LEARNING MODEL FOR FACIAL EXPRESSION CLASSIFICATION**\n",
        " **Resnet18 or 50**\n",
        " * ResNet-18 is a shallower and lighter model, suitable for tasks where computational resources are limited or when a simpler model architecture suffices. On the other hand, ResNet-50 is deeper and more powerful, offering higher capacity and potentially better performance on complex tasks and larger datasets, albeit at the cost of increased computational requirements and training time.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-yV0Ql0RfKK"
      },
      "outputs": [],
      "source": [
        "input_shape = (96, 96, 1)\n",
        "\n",
        "# input tensor shape\n",
        "X_input = Input(shape=input_shape)\n",
        "\n",
        "# Zero padding\n",
        "X = ZeroPadding2D((3,3))(X_input)\n",
        "\n",
        "# 1st Stage\n",
        "X = Conv2D(64, (7,7), strides = (2,2), name = 'conv1', kernel_initializer = glorot_uniform(seed = 0))(X)\n",
        "X = BatchNormalization(axis = 3, name='bn_conv1')(X)\n",
        "X = Activation('relu')(X)\n",
        "X = MaxPooling2D(pool_size=(3,3), strides=(2,2))(X)\n",
        "\n",
        "# 2nd stage\n",
        "X = res_block(X, filter=[64, 64, 256], stage=2)\n",
        "\n",
        "# 3rd stage\n",
        "X = res_block(X, filter=(128, 128, 512), stage=3)\n",
        "\n",
        "# Average Pooling\n",
        "X = AveragePooling2D(pool_size=(4,4), name='Average_Pooling')(X)\n",
        "\n",
        "# Final Layer\n",
        "X = Flatten()(X)\n",
        "X = Dense(units=5, activation='softmax', name= 'Dense_final', kernel_initializer= glorot_uniform(seed=0))(X)\n",
        "# Here, in Dense layer, we only provide 5 output as we have in the data as emotion type\n",
        "\n",
        "model_2_emotion = Model(inputs= X_input, outputs= X, name='Resnet18')\n",
        "model_2_emotion.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N15RSGHoiAqZ"
      },
      "source": [
        "### COMPLIE & TRAIN KEY FACIAL POINTS DETECTION deep learning model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdOw360edqUW"
      },
      "outputs": [],
      "source": [
        "adam = tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    weight_decay=None,\n",
        "    clipnorm=None,\n",
        "    clipvalue=None,\n",
        "    global_clipnorm=None,\n",
        "    use_ema=False,\n",
        "    ema_momentum=0.99,\n",
        "    name='adam'\n",
        "  )\n",
        "model_2_emotion.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgeznXr1xw0m"
      },
      "source": [
        "**Early Stopping:**\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss',  # Monitor validation loss\n",
        "                               patience=5,          # Number of epochs with no improvement after which training will be stopped\n",
        "                               restore_best_weights=True)  # Restore model weights from the epoch with the best value of the monitored quantity\n",
        "\n",
        "* monitor='val_loss': This indicates that the validation loss will be monitored for early stopping. Training will stop if the validation loss stops decreasing.\n",
        "\n",
        "* restore_best_weights=True: This ensures that the model weights are restored to the best observed weights during training when training is stopped early. It helps prevent overfitting by reverting to the model state that performed best on the validation set.\n",
        "\n",
        "* The .h5 extension is commonly used for saving models in HDF5 format, which is a popular file format for storing large numerical datasets. The .keras extension could be used to indicate that the file contains a model saved using Keras, but it's not a standard convention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CVNZLT1vmZs"
      },
      "outputs": [],
      "source": [
        "# using early stopping to exit training if validation loss is not decreasing even after certain epochs (Patience)\n",
        "earlystopping = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose=1, patience=20, restore_best_weights=True)\n",
        "\n",
        "# save the best model with lower validation loss\n",
        "checkpointer = ModelCheckpoint(filepath = \"FacialExpression_weights.h5\", monitor='val_loss', mode='min',  verbose=1 , save_best_only = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y3oLQYuZDWUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnPMj_RPvmdL"
      },
      "outputs": [],
      "source": [
        "history = model_2_emotion.fit(train_datagen.flow(X_train, y_train, batch_size=32), validation_data=(X_val, y_val), steps_per_epoch=len(X_train)//64, epochs= 100, callbacks=[checkpointer, earlystopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIV5w6ctvnch"
      },
      "outputs": [],
      "source": [
        "# saving the model architecture to json file for future use\n",
        "model_json = model_2_emotion.to_json()\n",
        "with open(\"FacialExpression-model.json\", \"w\") as json_file:\n",
        "  json_file.write(model_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmLwYq5UvnpR"
      },
      "outputs": [],
      "source": [
        "# Load the model architecture from the JSON file\n",
        "with open(\"FacialExpression-model.json\", \"r\") as json_file:\n",
        "    loaded_model_json = json_file.read()\n",
        "\n",
        "# Reconstruct the model from the loaded architecture (Loaded_model = model_1_facialKeyPoint ho)\n",
        "model_2_emotion = model_from_json(loaded_model_json)\n",
        "\n",
        "# Load the trained weights\n",
        "model_2_emotion.load_weights(\"weights_emotions.h5\")\n",
        "\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "# Compile the loaded model (if needed)\n",
        "model_2_emotion.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Use the loaded model for inference or further training\n",
        "# For example:\n",
        "# predictions = model_1_facialKeyPoint.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = model_2_emotion.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy: \", score[1])"
      ],
      "metadata": {
        "id": "aI-2U8U5JGzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history.history.keys()"
      ],
      "metadata": {
        "id": "OYnmrzzAJoHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']"
      ],
      "metadata": {
        "id": "euxeEi8wJoRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(len(accuracy))\n",
        "\n",
        "plt.plot(epochs, accuaracy, 'bo', label='Training Accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'b', label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "rRv0LhQHJ2vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nsQz6TH4J2y7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1aqBS_I8Px-eUTQ7xHvkBkdyC20yLdPD7",
      "authorship_tag": "ABX9TyNP5GLDxVMnJ0zfv2NBc/TK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}